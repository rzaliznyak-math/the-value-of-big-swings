---
title: "The Value of Big Swings"
subtitle: "Why Playing It Safe in A/B Testing Might Be Holding You Back"
author: "Russ Zaliznyak <[rzaliznyak@gmail.com](mailto:rzaliznyak@gmail.com)>"
date: "2025-08-25"
execute: 
  echo: false
format: 
  html: 
    toc: true
    toc-expand: true
    toc-indent: true
---

# TL/DR

A/B testing is a tool for driving business outcomes like conversion, retention, and revenue. But in practice, experimenters often face a strategic dilemma:

- **Play it safe** with frequent, incremental wins  
- **Swing big** with fewer, riskier tests that promise outsized returns  

This tradeoff is well-known in the experimentation community â€” a recurring challenge when traffic is limited and the stakes are high.

This paper compares two strategies:

- **Guaranteed Small Wins** â€” ten low-risk bets with small, consistent gains  
- **Transformational Tests** â€” a single high-variance bet that could flop â€” or deliver a breakthrough

Which strategy delivers more value on average?


---

# Experiment Design 

To ground this comparison in a concrete scenario, I define a base experiment with the following parameters:

| Min (Test Days) | Max (Test Days) | Max (Test N) | Post Test N | Baseline | Split |
|-----------------|-----------------|--------------|-------------|----------|-------|
| 7               | 20              | 80,000       | 500,000     | 65%      | 50/50 |

<br>
This setup reflects a common A/B testing environment: experiments run between 7 and 20 days, with a capped sample size during the test phase, followed by rollout to a larger post-test population.

To evaluate strategic tradeoffs, I simulate two competing approaches:

1. **Guaranteed Small Wins**: Ten low-risk experiments, each drawing from a tightly clustered distribution of small positive effects.
2. **One Transformational Experiment**: A single, riskier test drawing from a broader distribution with higher variance, including both harmful and high-upside ideas.


<!-- One-time styles -->
<style>
img.framed { border:1px solid #ddd; border-radius:8px; padding:6px; }
.tile-label { text-align:center; font-weight:600; margin-bottom:6px; }
.tile-sub { text-align:center; font-style:italic; font-size:0.9em; margin-bottom:10px; }
</style>

::: {.columns}
::: {.column width="50%"}
<div class="tile-label">Distribution of Guaranteed Small Wins</div>
![](./assets/small.png){fig-alt="Expected treatment effects, small wins" width="100%" .framed}

- Effects densely packed around 100.1  
- Only small positive effects
:::

::: {.column width="50%"}
<div class="tile-label">Distribution of Transformational Experiment</div>
![](./assets/big.png){fig-alt="Expected treatment effects, transformational" width="100%" .framed}

- Î¼ = 99.5, Ïƒ = 2.25  
- 60% harmful ideas  
- High upside potential
:::
:::


# Simulation Framework

To evaluate these strategies, I simulate **30,000 randomized A/B tests** in each of two distinct distributions: one representing **Guaranteed Wins**, with tightly clustered positive effects just above baseline, and one representing **Transformational Ideas**, with a wider spread, higher upside, and meaningful downside.

Each simulation samples a random true effect from its distribution, while decisions are made using noisy, observed outcomes â€” mirroring real-world test uncertainty. Final comparisons focus on average marginal impact.

## Definition

A measure that quantifies the risk of making the wrong decision, combining both the likelihood of harm and its expected magnitude:

$$
\text{Expected Loss} = \Pr(\text{harm}) \times \mathbb{E}[\text{Magnitude of Harm}]
$$

Weâ€™ll refer to expected loss simply as **`Risk`**. This is used as a stopping criteria below.


## Evaluation Approach

1. For each simulation, and each day on or after `Min (Test Days)`:
   - Compute **Risk (expected loss)** for both control and treatment conditions.
   - Compare each conditionâ€™s **Risk** to the **Risk Threshold = 0.025%**.

2. Decision rules:
   - If **only one** condition breaches the threshold â†’ select it as the winner.
   - If **both** breach â†’ choose the condition with the lower risk.
   - If **neither** breaches by the end of the test â†’ select the condition with the lowest final risk.

3. After selecting a winner:
   - Simulate rolling out the winner to the remaining test population and the post-test population, regardless of whether it was truly best.

## Results

All metrics shown are based on 30,000 simulations and represent expected values, i.e., average outcomes aggregated across all simulated experiments --- with each simulation drawing a random treatment effect from its distribution of effects.

| Metric                        | One Guaranteed Win Experiment | One Transformational Experiment |
|------------------------------|-------------------------------|---------------------------------|
| **Risk Threshold**           | 0.025%                        | 0.025%                          |
| **Selected Best Condition**  | 57.69%                        | 58.56%                          |
| **Marginal Events vs No Test** | 216                          | 2,261                           |
| **Projected Mean @ Year-End** | 65.037%                      | 65.390%                         |
| **Mean Bayesian Test**       | N = 59,516 (15 days)          | N = 38,713 (10 days)            |


<br>

On average, ten guaranteed-win experiments produce ~2,160 marginal events â€” but require ~5.8 million users across the full test and post-test lifecycle.<br>
One transformational experiment delivers a comparable ~2,261 marginal events on average, using just 580k users and completing 33% faster.

While both strategies yield similar impact on average, transformational testing is far more efficient â€” achieving comparable value with one-tenth the traffic on average.

# Home Runs Outweigh Strikeouts

How can a strategy with a 60% strikeout rate still outperform?

Because when a transformational idea hits, it hits big. Risk-based testing helps cut losses early while allowing strong performers to scale â€” letting a single home run outweigh several duds.

Thatâ€™s the asymmetric nature of upside: most variants may underperform, but the rare wins more than make up the difference.



```{python}

import plotly.graph_objects as go

# Data
itc_values = [97, 98, 99, 99.5, 100.5, 101, 102]
conversions = [-274, -200, -250, -390, 1500, 3500, 7300]
labels = [str(v) for v in itc_values]  # Convert to strings for categorical x-axis
colors = ['red' if val < 0 else 'green' for val in conversions]

# Create bar chart
fig = go.Figure(data=[
    go.Bar(
        x=labels,
        y=conversions,
        marker_color=colors,
        text=[f'{val:+}' for val in conversions],
        textposition='outside'
    )
])

# Layout settings
fig.update_layout(
    title="Marginal Events by Index-to-Control (ITC) Value",
    xaxis_title="ITC Value",
    yaxis_title="Marginal Events (Conversions)",
    template='plotly_white',
    yaxis=dict(zeroline=True, zerolinecolor='black'),
    bargap=0.3
)

fig.show()



```

<!--
| ITC Value   | 97     | 98     | 99     | 99.5   | 100.5  | 101    | 102    |
|-------------|--------|--------|--------|--------|--------|--------|--------|
| Marginal Events | -274   | -200   | -250   | -400   | +1500  | +3500  | +7300  |
-->

<br>

Risk-based testing makes transformational strategies more efficient and impactful â€” a single high-performing variant can easily outweigh multiple duds, justifying the risk.


# Conclusion

Both strategies yield similar marginal impact on average â€” but transformational testing is **far more efficient**, requiring fewer users, less time, and fewer tests to achieve comparable results.

While riskier, the upside of transformational experiments dramatically outweighs the downside when paired with a smart decision framework. In environments where time and traffic are limited, this efficiency becomes a strategic advantage.

# Recommendation

**Favor transformational tests when possible** because they unlock greater value per user and per test.  

That said, a **hybrid approach** can provide balance: guaranteed wins deliver stability and coverage, while bold swings create room for breakthrough gains. Together, they offer a path that scales.

---




::: {.callout-note collapse="false" icon="ðŸ’¬" title="<b>About Me</b>"}

<div style="font-size: 110%">


I work on experimentation systems, Bayesian optimization, and simulation-driven decision-making. My recent work includes Bayesian long-term optimization, informed priors, and CUPED.

If you're thinking about experimentation strategy, simulation design, or testing frameworks â€” I'd love to connect. Always happy to trade ideas or dig deeper into what drives meaningful impact.

ðŸ“© **Email:** [rzaliznyak@gmail.com](mailto:rzaliznyak@gmail.com)

</div>

:::



<!--



::: {.callout-note}
### Prompt for Reflection
- Do you prefer the steady compounding of **small, safe wins** or the risky **big swing** with transformational potential?  
- How should your organization weigh **risk vs. upside** when running AB tests?  
:::
-->